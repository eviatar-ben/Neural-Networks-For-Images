{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Original\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import wandb\n",
    "\n",
    "# WB = False\n",
    "WB = True\n",
    "\n",
    "PLOT = not WB\n",
    "LOSSES = [\"Original\", \"Non-saturation\", \"L2\"]\n",
    "LOSS = LOSSES[0]\n",
    "RUN = f'GAN_{LOSS}_'\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "G_D_FACTOR = 4\n",
    "DESCRIPTION = f\"Loss Saturation_GD_FACTOR{G_D_FACTOR}\"\n",
    "D = 15\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# input noise dimension\n",
    "nz = 100\n",
    "\n",
    "# number of gpu's available\n",
    "ngpu = 1\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, nc=1, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, nc=1, nz=100, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "def init_w_and_b(group, project, d=\"\", description=\"\", run=\"\", LR=1e-3, EPOCHS=10, architecture=\"GAN\"):\n",
    "    wandb.init(\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "        # Set the project where this run will be logged\n",
    "        group=group,\n",
    "        project=project,\n",
    "        name=f\"{description}{run}{d}\",\n",
    "        notes='',\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": LR,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": \"MNIST\",\n",
    "            \"epochs\": EPOCHS,\n",
    "        })\n",
    "\n",
    "def load_data(batch_size=64):\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "\n",
    "    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def train():\n",
    "    dataloader, _ = load_data()\n",
    "    criterion = nn.BCELoss() #todo: maybe logit is required?\n",
    "\n",
    "    netD = Discriminator(ngpu).to(device)\n",
    "    netG = Generator(ngpu).to(device)\n",
    "\n",
    "    optimizerD = torch.optim.Adam(netD.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "    optimizerG = torch.optim.Adam(netG.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            #############################################################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ############################################################\n",
    "            # train D with real\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            batch_size = real_cpu.size(0)\n",
    "            label = torch.full((batch_size,), real_label, device=device).to(torch.float32)\n",
    "\n",
    "            output = netD(real_cpu)\n",
    "            # output = output.to(torch.float32)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # train D with fake\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################################################\n",
    "            # (2) Update G network:\n",
    "            # i.  minimize log(1-D(G(z))) equals maximize -log(1-D(G(z)))\n",
    "            # ii. minimize -log(D(G(z))) equals maximize log(D(G(z)))\n",
    "            ############################################################\n",
    "            # train G\n",
    "            if i % G_D_FACTOR == 0:\n",
    "                netG.zero_grad()\n",
    "                label.fill_(real_label)\n",
    "                output = netD(fake)\n",
    "                errG = criterion(output, label)\n",
    "                errG.backward()\n",
    "                D_G_z2 = output.mean().item()\n",
    "                optimizerG.step()\n",
    "\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, EPOCHS, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        if WB:\n",
    "            wandb.log({\"Loss_D\": errD.item(), 'Loss_G': errG.item(),},\n",
    "                      step=epoch)\n",
    "            # wandb.log({\"Loss_D\": errD.item(), 'Loss_G': errG.item(),\n",
    "            #            \"D(x)\":D_x, \"D(G(z))\":D_G_z2 }, step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    if WB:\n",
    "        init_w_and_b(project=\"NN4I_Ex3 \", group=\"Loss Saturation\",\n",
    "                           d=DESCRIPTION, description=DESCRIPTION, run=RUN, LR=LR,\n",
    "                           EPOCHS=EPOCHS, architecture=\"GAN\")\n",
    "    train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
